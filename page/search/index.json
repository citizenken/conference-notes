[{"content":"","href":"/conference-notes/","title":"Home"},{"content":"Codebuild builder session * pay by minute * infinite scale * no queue times * docker based\nJenkins plugin for codebuild to move the build step to AWS\nhttps://github.com/johnhanks1/jekyll_example\n","href":"/conference-notes/aws_reinvent_2018/codebuild_builder_session/","title":""},{"content":"","href":"/conference-notes/aws_reinvent_2018/multicloud_myth/","title":""},{"content":" How Amazon releases Mission-critical Software  executes thousands of deploys at any give second 99.996% of deploys execute without problems always pushing to 100%\n much of what enables Amazon do do ^ is in our tooling\n  Tech and cultural landscape at Amazon  two pizza teams 8 engineers is still the average size at Amazon  communication cost is expensive  end to end ownership Amazon is a \u0026ldquo;federation of startups\u0026rdquo;  given a domain, most decisions are the teams teams pick tech stack, ratio of roles, own service end to end  Amazon\u0026rsquo;s service oriented architecture  services have public contract only way other services talk to you allows for constant refactoring behind the contract  Local dev environment is the wild west  Agile-ish \u0026ldquo;The Process\u0026rdquo; the release/debug process/etc. will ensure you don\u0026rsquo;t cut corners iterate as quickly as you can your host is a service  local service running is easy local dev can interact with other services out in the different environments that you have   With all these services, mangement is tough\n tolling helps organize all of that focus on foundational tools, the building blocks first  ex. S3, EC2, then more complex serices on top  internal marketplace  share tooling/practices/solutions wait for the best of the ^ ideas to bubble to the top the best are then devloped on top of and \u0026ldquo;funded,\u0026rdquo; resources are allocated   their pipelines are aggressively pessimistic\n to prevent changes from getting stuck in the process, ie. integration stuff lasting around being harsh on changes makes sure that only the best  Git in Amazon\n Called Gitfarm  replication for free finetuned access control, file by file, etc. code search on commit CodeCommit has many of those features    Tooling philosophy Code review, build and the pre-mortem  rules on top of commits to mainline based\n rules like compliance as early as possible certain files require certain reviewers  build process\n global dependency closure  parent POMs? ex. 1 version of Spring for the whole company hyper aware of who is using what  ex. if a dependency is compromised, easy to remove that version   unit testing static analysis  the pre-mortem\n \u0026ldquo;correction of errors\u0026rdquo;  document all the bad ways the software could cause problems  what is the worst thing that could happen how do we mitigate it how do we fix it   don\u0026rsquo;t do this process all the time large changes sensitive code areas\nPipelining and deployment  3 envs\n test env: wild west integration: configured and looks like prod \u0026lt;- interesting  called public contracts for dependent services prevents having a unique env  production  rules around releasing w/regions and AZs to prevent widespread failures\n picture: columns are stages of the release pipeline, depth is the tests that are running\n  ","href":"/conference-notes/aws_reinvent_2018/releasing_mission_critical_software/","title":""},{"content":"Severless patterns notes: * traits of serverless: 1. no server or container mgmt 2. flexible scaling (horizontal is taken care of) 3. HA, in all AZs 4. dont pay for idle\n lambda execution\n dl code state new container bootstrap the runtime (java/ruby/python, etc. environtment) run the code  steps 1-3 are \u0026ldquo;cold start\u0026rdquo; so there is a delay, but subsequent requests use the warmed containers\n tuning function resource helps with costs and cold/warm uses\n increasing memory usage helps with costs\n store secrets in env vars, encrypted in KMS\n  Pattern 1: * static files hosted on S3, cached by cloudfront * api gateway as a backend, sitting in front of lambda, which sends data to a store (dynamodb) * signing is cognito\n api gateway\n edge optimiazed - sits at the end of the cloud,  validated json web tokens for auth reform urls implement securit headers  regional * private  just available internally   lambda@edge - edge-based processing\n cost can be broken down to the activity level (how much an order costs to process)\n  ","href":"/conference-notes/aws_reinvent_2018/serverless_patterns/","title":""},{"content":"These are some notes\n","href":"/conference-notes/aws_reinvent_2018/","title":"AWS Re:Invent 2018"},{"content":"   pluses * streamlined devops, codepipeline * supports a lot of languages * tag direct costs to projects\nconsiderations: * know what specialties are * certain languages not supported * need to deal with cold starts, concurrent read/writes, etc. * migration from non-aws requires focus and commitment\n kinesis firehose to watch for inbound connections faster than humans could do, and then try to shut down (ex. using \u0026ldquo;i am not a robot\u0026rdquo;)\n the serverless firehose can take that logging stream, analyze, and store it example used was bots trying to auth another example was comcast logins from a partner that had a retry bug (like hbo login using comcast)  blowing up traffic w/requests, serverless prevented other services from having performance hits   comcast primetime tv has huge scale issues (like firing off recordings, people turning it on, etc.)\n comcast netflix had huge requirements for logins/auth, so they had to make a VPC that could handle those requests, as well as could talk to the on prem systems\n  ","href":"/conference-notes/aws_reinvent_2018/comcast/","title":"Accelerate Innovation \u0026 Maximize Business Value with Serverless Apps [SRV212]"},{"content":"   cd best practices * VCS * automated builds * automated deployments * deploy to \u0026gt; 1 instances (HA, spread load) * unit tests * integration tests (selenium, functional, syntentic users) * deliver * op Tools in the talk - AWS specific * monitoring - cloudwatch * sns - notifications * lambda * aws codedeploy/codepipeline for delivery * DEV309 - serverless + containers codedeploy/codepipeline\nAdd saftey to rolling deployments * check health * ensure minimum instances * rollback\nCanaries * segment environment * test * use codepipeline wait for approval step * deploy to a segment * fire lambda that enters meta data into dashboard * triggers another lambda that monitors if enough usage has happened, time has passed, no errors have happened * send approval to the pipeline\n","href":"/conference-notes/aws_reinvent_2018/advanced_cd_best_practices/","title":"Advanced Continuous Delivery Best Practices [DEV317]"},{"content":" flexible schema much easier to deal with when using serverless  don\u0026rsquo;t need to deal with updates  lambda@the edge is like lamdba, but at the edge of your network, so it can do smarter routing to resources for ex.  for ex. feature flags to give beta version to a particular company  when you update a lambda, it creates an immutable version ID\n Aliases can be defined and linked to version IDs Allows for ex. evens to go to particular versions allows for weight between versions  ex. prod points 95% of invocations @ v1, 5% @ v2, for canary testing invocations are distributed in a round-robin fashion  in SAM, can configure deployment strategy to funtions  allows automated weight manipulation if no errors, keep ramping up (10 percent, increasing every 10 minutes for ex.) requires monitoring  hooks and alarms (Cloudwatch alarm) suggests to link canary metrics to customer action rather than just infrastructure pre and post traffic functions that calls back to code deploy to accept/reject version    similar weights can be added to API gateway to send traffic to certain API\n  Life stages of Lambda development\nV1: * User manually configures the function in the UI * doesn\u0026rsquo;t allow multiple users * not saved outside of the UI V2: * Something in git changes, that updates the lambda V3: * Similar to V2, but has multiple environments, with testing before prod V4: * similar to V3, but the testing portion is build out into canary deployments * pre traffic hooks can fail deployments, so customers don\u0026rsquo;t see any errors * once the canaries are happy, 100% of traffic is on the new version\nWhy would we move to serverless when things are working currently * serverless is easy to create a sandbox environment for external developer to play with * just define another SAM configuration * more scalable because it\u0026rsquo;s there automatically * no idle capacity * no server admin\n best practices\n function duration control your retries know your limits use versioning amazon cloudwatch - insert logging statements! protect your Lambda functions with IAM roles  even like allowing only access to certain fields in a dynamo table  plan for dependencies when setting timeout value test test test - best way to turn functions is by testing  evaluating the move to serverless is a good time to review current design and architecture\n ie. why use postgres if most queries are just lookups  serverless and databases\n know your data know the available cost model and match it to your needs tests at scale encryption secure your flow - IAM roles  lessons learned from journey to serverless\n think microservices canary deployments cold start  not as much of a problem as ppl say aws adjusts cold starts over life of the function, learns about it review design if cold starts are impacting you, ie. function runs once a day  yes/no functions  keep functions simple  events source distributed tracing environmental variables  include this kind of stuff ^ in your MVP\n  Why does Capital One use Go? * Go\u0026rsquo;s native concurrency is a boon for network applications that live and die on concurrency * modern apps are designed to be cloud-native and to take advantage of loosely coupled cloud services\n","href":"/conference-notes/aws_reinvent_2018/safe_serverless_deployments/","title":"Best Practices for Safe Deployments on AWS Lambda and Amazon API Gateway [SRV343]"},{"content":"   Key Insights  measure and improve human, organizational and machine systems  measure everything always  SRE is a move from reactive to proactive event mgmt  eliminate toil and eliminate them  toil = manual operational work that doesn\u0026rsquo;t scale   Provide organizational back pressure mechanism  delay features to clean up tech debt don\u0026rsquo;t deploy new code until the app is more reliable push back to teams/the org feedback loop   How to convert ppl * set early expectations * have to get C-level buy in * takes time\nCoinbase * how do we improve reliability if we don\u0026rsquo;t know where we\u0026rsquo;re reliable today * installed service-level instrumentation * used \u0026ldquo;Four Golden Signals\u0026rdquo; to determine where to start that instrumentation initiative * latency * direct impact on customer experience * where and how you measure it is important * load balancer? server-side? client-side for round trip? * traffic * transactions per second * direct relationship with business value * most companies/services have a threshold above which we don\u0026rsquo;t know what happens * errors * typically expressed as a ratio, errors/time * helps set a nice target for improvement * easily digestible * another direct impact on customer experience * goal is not to eliminate errors, but find an acceptable error rate * saturation * how close are you to total available * how much DU and memory usage * its an issue everywhere at all time\n Golden Signals can apply to human organizations, like teams\n burnout \u0026lt;- saturation good SRE should be able to track this as well blockers \u0026lt;- latency how many tickets in a sprint \u0026lt;- traffic humans error \u0026lt;- errors  start somewhere, a best guess, and iterate from there\n a spreadsheet  column for services, then one for each golden signal, and what that service uses for that signal  define \u0026ldquo;done\u0026rdquo;  ex. per-service dashboard in datadog for timeseries chart for each indicator document describing the different indicators and why they are important  a spec document is important  helps guide implementation where do you want to instrument? client/load balancer/etc. why indicator matters what it tells you where you\u0026rsquo;re going with it having this documentation embedded in the tool is critical so it stays up to date  once we have some of these things defined, then we can define SLIs/promises  plain-language statements, easy to parse, easy to understand plenty of potential stateholders \u0026lt;- bring them onboard at this point we do all this so we can keep promises, with customers, etc.   more on promises\n Thinking in Promises by Mark Burgess promises have two parties promises can be human/machine, machine/machine, etc.  cross-functional team members can add more insight to different types of promises  example promises  your service promises to respond to client within 50ms a service you depend on promises that its error rate will be \u0026lt; 1% on-call promises they will engage an incident within 15 minutes  When are promises done?  promises are done when they have a monitor alert forecasting - your tooling should be able to analyze and look historically to predict failures  ID stuff that runs the risk of breaking a promise and fix before it breaks promises   when promises are broken\u0026hellip;  its inevitable, so plan for that inevitability it builds trust   blame-less post-mortems\n let the data do the talking requires 3 things  good instrumentation good understanding of what you\u0026rsquo;re measuring process for interpreting   interpreting incidents\n build a shared language practice communication understand that incidents and outages are broken promises creates domain-specific language with no ambiguity, quickly  measure incident response\n quantify and measure the quality of ryour incident responses quantitative: time to detect, time to engage, time to fix qualitative: quality of communication  it is clear who to talk to do we have all the stakeholders documented   RECAP\n why SRE?  why gets your buy in  get instrumentation started  spreadsheet, documents  promise enumeration  document clearly what our promises is  measure response when promises are broken gains  transparancy *    ","href":"/conference-notes/aws_reinvent_2018/coinbase_sre/","title":"Building SRE from Scratch at Coinbase during Hypergrowth [DEV315]"},{"content":"","href":"/conference-notes/categories/","title":"Categories"},{"content":"","href":"/conference-notes/tags/cd/","title":"Cd"},{"content":"","href":"/conference-notes/tags/codepipeline/","title":"Codepipeline"},{"content":"","href":"/conference-notes/tags/culture/","title":"Culture"},{"content":"","href":"/conference-notes/tags/customer-story/","title":"Customer Story"},{"content":"","href":"/conference-notes/tags/devops/","title":"Devops"},{"content":"    50 million deploys/year tools start to dictate behavior cultural philosphy  how often do we want to interact with our customers   timeline\n 2001 amazon was a monolith monolith had a lot of interdependencies went back and isolated independent units  cart stock labeling  each unit gets 2 pizza teams after breaking up the units, agile was easier creating a multi-displinary team is NOT devops  each team member should have multiple skills, with fullstack ownership learn new things, compliment team  everything is code, so everything can be tested guardrails for self-service \u0026lt;\u0026ndash; key idea  opinionated templates helps bootstrap quickly without falling off the cliff   ","href":"/conference-notes/aws_reinvent_2018/amazon_devops/","title":"Moving to DevOps the Amazon Way [DEV210]"},{"content":"","href":"/conference-notes/page/","title":"Pages"},{"content":"    small companies have small Teams\n as it grows, central teams develop (ex. tools, platfomrm, infra)\n becomes a \u0026ldquo;fractal\u0026rdquo; of central teams\n different BUs have central teams that talk to a central time  moving from traditional dev/ops/qa model to full end-end ownership\n with DevOps teams, why do we need central teams?\n most devops teams, while their work is different, they have the same software development lifecycle\n how many different implementations of monitoring/building tooling/etc. across a company?\n His opinion: as few as possible  Central teams create these common workflows and processes and abstract a lot of the process stuff away from devs\n no central team mandate at Netflix\n no one has to use their tooling challenge, but also a value, because they are providing the best-of-breed solution  rate of adoption becomes important metric is our stuff being used? if not, lets find out whats going wrong   what are you optimizing?\n availability, velocity, efficiency need to align with the other teams on these factors ex. startups velocity is more important vs. bigger companies who care about availability  leader or fast follower? central teams need to determine that\n dangers of getting too far ahead or too far behind to far ahead of teams? can lead to churn amongst internal customer, too many different solutions too fast to far behind? customers will make their own solutions/shadow IT  happy medium between leading/follower of treads amongst teams\n work with pioneering teams to understand what their solution is early, evaluate scalability once there is an inflection point, that other teams want to use the solution, that can be a good time to move to a central team  Build or buy?\n when do you build a bespoke solution instead of leveraging an existing one? This is their decision model:  look at AWS, see if there is something look at other 3rd parties look at opensource. Even if it only statisfies like, 80% of use cases, might still be interesting, because you can contribute upstream roll it your own if there is really no other solution    two opensource stories: 1. spinnaker - popular, good community support, Google/AWS contribution 2. OSS - failed, didn\u0026rsquo;t take off\n central team invests in educaion\n onboarding so best practices and patterns are in from the start continuous education to bring existing ppl onboard  good discoverabily for tooling resources\n if there isn\u0026rsquo;t strong discoverabily, ppl won\u0026rsquo;t use your solution  how to continuously update your customers on what\u0026rsquo;s new and improved?\n figure out what kind of avenue is effective?  brown bag lunches newsletters road shows onboarding sessions/continuous sessions  takes all types of communitation all the time  generalized vs. specialized needs\n too generalized leads to least-common-denominator too specific and it can\u0026rsquo;t be shared well solution? try to find a happy medium, then  make their internal tooling to be extensible inner opensource   for a long time, they had no dedicated product management\n start out as team doing customer feedback, requirement gathering, prioritizing at some point, there is a scale/complexity that PMs are needed  now they are trying to create a PM roll not only does what they did before, but can take on more and be more focused   finding the right engagement model\n working directly with teams creates a customer -\u0026gt; provider relationship working with distributed central teams makes them more partners than customers  they have a closer understanding to what their teams need, can help manage the general/specialized issue   driving change\n finding the key updates/migrations and prioritizing/pushing that adoption curve  early, fast, no problem medium once it is more mature not done until that tail is done  current just works has to decide what is good for a couple lagging teams vs. the company    align central teams\n many central teams  security, systems, data, etc.  have to align central teams before approaching customers prevents tons of \u0026ldquo;tiny\u0026rdquo; requests slowing things done  \u0026ldquo;Highly aligned, loosely coupled\u0026rdquo;\n Fragmentation:\n teams developing a solution that already exists causes fragmentation  accidental:  issue with developer awareness/outreach/discoverability  deliberate:  harder to address team know a solution exists, but rejects it important signal to determine shortcoming could be an awareness issue could be a real gap    celebrate adoption\n adoption is a key metric when the metric is met/good, need to recognize that  soft metrics are valuable\n hard metrics can sometimes prevent you from seeing the forest through the trees ex. customer happiness   QA * guardrails and how that works with central teams * different concept than gatekeeprs * surface potential issues as suggestions and \u0026ldquo;are you sure you want to do that\u0026rdquo; * ex. spinniker from making large changes during busiest times. Are you sure?\n how do you keep teams\u0026rsquo; trust and not becoming ticket hubs?\n constant engagement, documentation ask for feedback, get an idea of what teams need, and then circle back when you address that need  authority that enforces mandate for ?\n sometimes, specifically in the security domain, the central teams force something responsiblity that developers make good, sound decisions?  not using something thats java cause youre a ruby guy is not a good solution   how do you resolve conflict?\n tough, but still works on escalating to managers  in early stage of devops transformation, when do you allow teams to have more freedom?\n if the teams can handle the responsibility and trust of owning their own stuff, then they can make their own decisions apart from the central team  what is the feedback mechanism\n feedback isn\u0026rsquo;t going to be coming in voluntarily frequently reach out to teams to get feedback  surveys have gotten good responses in the last few years   size of central teams as it relates to the size of the engineering teams\n try to think of it as leverage, leverage being the ratio of value of the product vs. cost of the product  if they grow at the same rate, then they maintain leverage, but not providing more value if they grow faster, then they lose leverage, harder to justify their grown because the ROI is impacted best is slightly under the same rate, because they\u0026rsquo;ll provide more value with fewer resources   no goverance, no chargebacks\n only have one product  one trick for newsletters\n distill updates as small as possible  the most impactful, relevant changes  3-page newletters with every change gets thrown out  do you embbed central team members in other teams?\n central team members go work in app teams for 6\u0026frasl;9 months, get real customer hands-on experience at the same time, they bring the central team knowledge to the app team, closing gaps and awareness forms stronger bonds, which helps with conflict resolution  when to decide to scale the central team?\n combination of the signal/happiness of customers analyzing portfolio, and figure out what gaps try to run lean, not making stuff that doesn\u0026rsquo;t have a need up to each team manager w/that decision  w/lack of adoption, does the central team throw out their solution and supports the one that takes off?\n no prescribed answer centralize the product work with the teams to decide if they want it centralized or not  how to handle central team priorities? how do you balance the need between new tech and what customers want?\n its a balance try to be situationally aware and understand the landscape understand the primary business needs understand the tech trends understand immediate customer needs really good product managers come in here to make the hard choices  being deliberate and communicating it to the rest of the company   centralized security team\n have to work closely, because they often use the platform team to push out security changes   Ruslan Meshenberg @rusmeshenberg\n","href":"/conference-notes/aws_reinvent_2018/central_teams/","title":"Role of Central Teams in DevOps Organizations [DEV70]"},{"content":"","href":"/conference-notes/page/search/","title":"Search"},{"content":"","href":"/conference-notes/tags/serverless/","title":"Serverless"},{"content":"","href":"/conference-notes/tags/session/","title":"Session"},{"content":"","href":"/conference-notes/tags/sre/","title":"Sre"},{"content":"","href":"/conference-notes/tags/","title":"Tags"},{"content":"   This talk was ok. The presentor was a bit much. I left early to attend another session.\n","href":"/conference-notes/aws_reinvent_2018/real_time_operations/","title":"Unleash Team Productivity with Real-Time Operations [DEV203]"}]
